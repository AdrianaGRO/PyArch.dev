[
  {
    "id": 1,
    "title": "Cleaning 450k rows in 84 seconds with Python",
    "date": "2026-01-22",
    "tags": ["automation", "python", "data-engineering"],
    "published": true,
    "content": "# Cleaning 450k rows in 84 seconds with Python\n\nI recently handled a data consolidation project that highlights why Python is superior to Excel for large-scale data processing.\n\nMy client needed to merge sales, customer, and inventory data from 12 different sources by a tight deadline. The raw dataset contained over 450,000 rows of inconsistent entries.\n\n## The Technical Challenge\n\nThe input data presented several standard data quality issues:\n\n- **Volume:** 450,000+ rows across 12 separated Excel files.\n- **Schema Drift:** Inconsistent column naming (e.g., \"Customer\" vs \"Client\" vs \"Company\").\n- **Formatting:** Dates mixed between string formats (\"Dec 15th\") and datetime objects.\n- **Redundancy:** Approximately 5% of the dataset consisted of duplicate records.\n\nManual standardization was estimated to take ~3-4 weeks. I opted to build an automated pipeline instead.\n\n## The Python Solution\n\nI utilized `pandas` for ETL (Extract, Transform, Load) operations and `glob` for file handling. Here is the core logic of the script:\n\n```python\nimport pandas as pd\nimport glob\n\n# 1. Ingest Data\nfiles = glob.glob('client_data/*.xlsx')\ndfs = []\n\nfor file in files:\n    df = pd.read_excel(file)\n    \n    # 2. Normalize Schema\n    df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n    mapping = {'customer': 'client', 'company': 'client'}\n    df.rename(columns=mapping, inplace=True)\n    \n    dfs.append(df)\n\n# 3. Merge & Deduplicate\nmerged = pd.concat(dfs, ignore_index=True)\nmerged.drop_duplicates(subset=['client', 'product'], keep='first', inplace=True)\n\n# 4. Standardize Dates\nmerged['date'] = pd.to_datetime(merged['date'], errors='coerce')\n\n# 5. Export\nmerged.to_excel('clean_dashboard_data.xlsx', index=False)\n```\n\n## Results\n\n* **Runtime:** 84 seconds\n* **Data Integrity:** 23,000 duplicates identified and removed.\n* **Efficiency:** Reduced a 4-week manual timeline to <2 hours of coding and execution.\n\nThis script is now deployed as a monthly cron job for the client, ensuring their reporting dashboard is always up-to-date without human intervention."
  },
  {
    "id": 2,
    "title": "Moving from Excel to Code: Automating Client Reporting",
    "date": "2026-01-20",
    "tags": ["python", "reporting", "pandas"],
    "published": false,
    "content": "# Moving from Excel to Code: Automating Client Reporting\n\nManual reporting is often the bottleneck in client communication. For a long time, my workflow relied heavily on Excel, which required about 4 hours per report for data cleaning, formatting, and chart generation.\n\nWhen scaling to multiple clients, this manual overhead became unsustainable. I transitioned to a code-first approach to improve consistency and reduce delivery time.\n\n## The Old Workflow (Manual)\n\n* Export data from CSVs.\n* Manually clean and filter in Excel.\n* Create charts and copy-paste them into a final document.\n* **Total Time:** ~4 hours per report.\n* **Risk:** High probability of copy-paste errors or versioning issues.\n\n## The New Workflow (Automated)\n\nI built a Python based generator using `pandas` for data manipulation and `jinja2` for templating. \n\n### The Architecture\n\n1.  **Data Layer:** `pandas` reads raw CSVs and performs aggregations.\n2.  **Visualization:** `matplotlib` generates necessary trend lines and saves them as images.\n3.  **Presentation:** `jinja2` injects the stats and images into a pre-styled HTML template.\n4.  **Output:** The HTML is converted to a PDF ready for email.\n\n```python\n# Pseudocode for the pipeline\ndata = pd.read_csv('data.csv')\n\n# Generate assets\ncreate_revenue_chart(data)\n\n# Render report\ntemplate = Template(open('report.html').read())\nfinal_report = template.render(\n    total_revenue=data['revenue'].sum(),\n    chart_path='chart.png'\n)\n```\n\n## Outcome\n\nBy treating reports as code rather than documents:\n\n* **Time Reduction:** 15 minutes execution time (down from 4 hours).\n* **Consistency:** Every report follows the exact same styling rules.\n* **Version Control:** Templates are tracked in Git, allowing for easy rollbacks.\n\nAutomation doesn't just save time; it allows us to focus on analyzing the insights rather than formatting the cells."
  }
]