[
  {
    "id": 1,
    "title": "How I cleaned 450,000 rows of messy data in 84 seconds",
    "date": "2026-01-22",
    "tags": ["automation", "python", "data-engineering"],
    "published": true,
    "content": "# How I cleaned 450,000 rows of messy data in 84 seconds\n\nLast month, a client dropped 12 Excel files in my inbox with a simple question: \"Can you make sense of this by Thursday?\"\n\nInside those files? Over 450,000 rows of sales data, customer records, and inventory reports - all with different column names, inconsistent date formats, and thousands of duplicate entries.\n\nIf I'd tackled this manually in Excel, I'd still be clicking through spreadsheets three weeks later. Instead, I wrote a Python script that finished the job in 84 seconds.\n\n## What I was dealing with\n\nThe files came from different systems, and it showed:\n\n- **450,000+ rows** spread across 12 separate Excel files\n- **Column names that didn't match** - some files called it \"Customer,\" others used \"Client\" or \"Company\"\n- **Date formats all over the place** - everything from \"12/15/2024\" to \"Dec 15th\" to actual datetime objects\n- **About 23,000 duplicate records** hiding in the data (roughly 5% of the dataset)\n\nMy client's internal team estimated 3-4 weeks of manual work to standardize everything. I had three days.\n\n## The solution I built\n\nI knew pandas could handle this, so I opened up my editor and got to work. The core logic ended up being pretty straightforward:\n\n```python\nimport pandas as pd\nimport glob\n\n# Step 1: Load all the files\nfiles = glob.glob('client_data/*.xlsx')\ndfs = []\n\nfor file in files:\n    df = pd.read_excel(file)\n    \n    # Step 2: Standardize the column names\n    df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n    \n    # Handle the naming inconsistencies\n    mapping = {'customer': 'client', 'company': 'client'}\n    df.rename(columns=mapping, inplace=True)\n    \n    dfs.append(df)\n\n# Step 3: Merge everything and remove duplicates\nmerged = pd.concat(dfs, ignore_index=True)\nmerged.drop_duplicates(subset=['client', 'product'], keep='first', inplace=True)\n\n# Step 4: Fix all the date formats\nmerged['date'] = pd.to_datetime(merged['date'], errors='coerce')\n\n# Step 5: Export the clean data\nmerged.to_excel('clean_dashboard_data.xlsx', index=False)\n```\n\nI hit run and watched the output. 84 seconds later, I had a single clean file with 427,000 rows - those 23,000 duplicates were gone.\n\n## What actually happened\n\nThe script processed all 12 files, standardized every column name, converted all the date formats, and identified every duplicate entry. No manual clicking. No copy-paste errors. Just clean, consistent data ready for analysis.\n\nInstead of spending 3-4 weeks doing this by hand, I spent about two hours writing and testing the script. Then I had two full days to actually work with the data and build something useful for the client.\n\n## The part that keeps paying off\n\nHere's what made this even better: the script didn't just run once.\n\nI deployed it as a monthly cron job on the client's system. Now every month, new files drop into their folder, the script runs automatically, and their dashboard updates without anyone touching it.\n\nWhat used to be their biggest monthly headache - the part that took days of manual work - now just happens in the background. They don't even think about it anymore.\n\n## Why this matters\n\nLook, most businesses have data like this somewhere. Files that don't match. Systems that don't talk to each other. Reports that someone has to compile manually every week or month.\n\nYou don't need to be a Python expert to automate this stuff. You just need to recognize that \"this is how we've always done it\" isn't a good enough reason to keep doing it.\n\nThe math is simple: 84 seconds beats three weeks. Every single time.\n\n---\n\n*If you're dealing with messy data that eats up your time every month, automation might be simpler than you think. Sometimes it just takes seeing someone else do it first.*"
  },
  {
    "id": 2,
    "title": "Moving from Excel to Code: Automating Client Reporting",
    "date": "2026-01-20",
    "tags": ["python", "reporting", "pandas"],
    "published": false,
    "content": "# Moving from Excel to Code: Automating Client Reporting\n\nManual reporting is often the bottleneck in client communication. For a long time, my workflow relied heavily on Excel, which required about 4 hours per report for data cleaning, formatting, and chart generation.\n\nWhen scaling to multiple clients, this manual overhead became unsustainable. I transitioned to a code-first approach to improve consistency and reduce delivery time.\n\n## The Old Workflow (Manual)\n\n* Export data from CSVs.\n* Manually clean and filter in Excel.\n* Create charts and copy-paste them into a final document.\n* **Total Time:** ~4 hours per report.\n* **Risk:** High probability of copy-paste errors or versioning issues.\n\n## The New Workflow (Automated)\n\nI built a Python based generator using `pandas` for data manipulation and `jinja2` for templating. \n\n### The Architecture\n\n1.  **Data Layer:** `pandas` reads raw CSVs and performs aggregations.\n2.  **Visualization:** `matplotlib` generates necessary trend lines and saves them as images.\n3.  **Presentation:** `jinja2` injects the stats and images into a pre-styled HTML template.\n4.  **Output:** The HTML is converted to a PDF ready for email.\n\n```python\n# Pseudocode for the pipeline\ndata = pd.read_csv('data.csv')\n\n# Generate assets\ncreate_revenue_chart(data)\n\n# Render report\ntemplate = Template(open('report.html').read())\nfinal_report = template.render(\n    total_revenue=data['revenue'].sum(),\n    chart_path='chart.png'\n)\n```\n\n## Outcome\n\nBy treating reports as code rather than documents:\n\n* **Time Reduction:** 15 minutes execution time (down from 4 hours).\n* **Consistency:** Every report follows the exact same styling rules.\n* **Version Control:** Templates are tracked in Git, allowing for easy rollbacks.\n\nAutomation doesn't just save time; it allows us to focus on analyzing the insights rather than formatting the cells."
  }
]